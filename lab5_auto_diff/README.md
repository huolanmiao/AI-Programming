# Draft version for final project
- 将我写的卷积层和池化层CUDA实现，写成`Conv2D`和`MaxPooling`算子，以适配自动微分框架（实现for-loop版本池化层反向传播）
- 实现三种模型架构：两层线性层的`pure_linear`架构、一层卷积层两层线性层的`simple_conv`架构、`LeNet`模型架构。具体来讲，对于每种架构，需要在`set_structure`中增加模型权重，更改`forward`函数，在优化器中更新每一层的权重。

## pure_linear
```shell
python task1_optimizer.py --model pure_linear
```
| Epoch | Train Loss | Train Err | Test Loss | Test Err | Epoch Time |
|-------|------------|-----------|-----------|----------|------------|
| 0     | 0.22435    | 0.06470   | 0.22508   | 0.06670  | 0.94579    |
| 1     | 0.14339    | 0.04090   | 0.15038   | 0.04400  | 0.73822    |
| 2     | 0.10491    | 0.02972   | 0.11988   | 0.03450  | 0.73497    |
| 3     | 0.08210    | 0.02337   | 0.10400   | 0.03100  | 0.67366    |
| 4     | 0.06755    | 0.01910   | 0.09560   | 0.02900  | 0.66912    |
| 5     | 0.05743    | 0.01633   | 0.09076   | 0.02730  | 0.73319    |
| 6     | 0.05035    | 0.01430   | 0.08808   | 0.02680  | 0.69270    |
| 7     | 0.04512    | 0.01305   | 0.08606   | 0.02610  | 0.70101    |
| 8     | 0.04096    | 0.01138   | 0.08459   | 0.02540  | 0.67365    |
| 9     | 0.03789    | 0.01022   | 0.08348   | 0.02480  | 0.68011    |
| 10    | 0.03506    | 0.00897   | 0.08228   | 0.02400  | 0.69014    |
| 11    | 0.03228    | 0.00802   | 0.08065   | 0.02410  | 0.66409    |
| 12    | 0.02955    | 0.00692   | 0.07888   | 0.02290  | 0.67286    |
| 13    | 0.02747    | 0.00615   | 0.07734   | 0.02260  | 0.74242    |
| 14    | 0.02596    | 0.00552   | 0.07636   | 0.02260  | 0.69701    |
| 15    | 0.02488    | 0.00525   | 0.07582   | 0.02270  | 0.64889    |
| 16    | 0.02410    | 0.00498   | 0.07548   | 0.02210  | 0.65099    |
| 17    | 0.02358    | 0.00487   | 0.07537   | 0.02200  | 0.72380    |
| 18    | 0.02325    | 0.00473   | 0.07532   | 0.02200  | 0.68885    |
| 19    | 0.02304    | 0.00465   | 0.07533   | 0.02200  | 0.69484    |


## simple_conv
```shell
python task1_optimizer.py --model simple_conv
```
| Epoch | Train Loss | Train Err | Test Loss | Test Err | Epoch Time |
|-------|------------|-----------|-----------|----------|------------|
| 0     | 0.22450    | 0.06557   | 0.22500   | 0.06870  | 4.74313    |
| 1     | 0.14129    | 0.04207   | 0.14670   | 0.04590  | 3.82161    |
| 2     | 0.10393    | 0.03257   | 0.11608   | 0.03600  | 4.41619    |
| 3     | 0.07885    | 0.02458   | 0.09998   | 0.03100  | 3.74715    |
| 4     | 0.06369    | 0.02007   | 0.09137   | 0.02800  | 3.64345    |
| 5     | 0.05231    | 0.01648   | 0.08724   | 0.02600  | 4.47715    |
| 6     | 0.04462    | 0.01400   | 0.08595   | 0.02530  | 4.18614    |
| 7     | 0.03778    | 0.01160   | 0.08470   | 0.02450  | 3.89238    |
| 8     | 0.03306    | 0.01043   | 0.08597   | 0.02510  | 3.80926    |
| 9     | 0.02982    | 0.00972   | 0.08827   | 0.02500  | 3.72024    |
| 10    | 0.02689    | 0.00833   | 0.08952   | 0.02460  | 4.40014    |
| 11    | 0.02264    | 0.00687   | 0.08822   | 0.02400  | 3.86780    |
| 12    | 0.01881    | 0.00515   | 0.08613   | 0.02290  | 3.90654    |
| 13    | 0.01623    | 0.00425   | 0.08498   | 0.02230  | 3.68627    |
| 14    | 0.01437    | 0.00360   | 0.08466   | 0.02190  | 3.52239    |
| 15    | 0.01296    | 0.00313   | 0.08437   | 0.02200  | 3.93964    |
| 16    | 0.01189    | 0.00277   | 0.08417   | 0.02200  | 3.79612    |
| 17    | 0.01113    | 0.00247   | 0.08417   | 0.02170  | 3.68573    |
| 18    | 0.01063    | 0.00225   | 0.08426   | 0.02180  | 4.57585    |
| 19    | 0.01031    | 0.00215   | 0.08432   | 0.02180  | 3.90759    |


## LeNet
```shell
python task1_optimizer.py --model LeNet
```

| Epoch | Train Loss | Train Err | Test Loss | Test Err | Epoch Time |
|-------|------------|-----------|-----------|----------|------------|
| 0     | 0.12592    | 0.04002   | 0.11623   | 0.03650  | 273.73302  |
| 1     | 0.07156    | 0.02250   | 0.07054   | 0.02180  | 271.74698  |
| 2     | 0.05404    | 0.01745   | 0.05941   | 0.02020  | 272.00233  |
| 3     | 0.05036    | 0.01662   | 0.06030   | 0.01930  | 272.11799  |
| 4     | 0.03622    | 0.01162   | 0.05037   | 0.01680  | 272.21837  |
| 5     | 0.03278    | 0.01103   | 0.05229   | 0.01790  | 259.61811  |
| 6     | 0.02956    | 0.01058   | 0.05080   | 0.01600  | 225.49822  |
| 7     | 0.02046    | 0.00718   | 0.04375   | 0.01330  | 227.44083  |
| 8     | 0.01514    | 0.00523   | 0.04190   | 0.01190  | 228.30170  |
| 9     | 0.01241    | 0.00428   | 0.04323   | 0.01140  | 229.07684  |
| 10    | 0.00949    | 0.00317   | 0.04298   | 0.01080  | 225.78679  |
| 11    | 0.00867    | 0.00290   | 0.04391   | 0.01150  | 228.54179  |
| 12    | 0.00779    | 0.00245   | 0.04425   | 0.01120  | 232.68834  |
| 13    | 0.00618    | 0.00183   | 0.04338   | 0.01130  | 226.14438  |
| 14    | 0.00507    | 0.00148   | 0.04325   | 0.01060  | 228.71153  |
| 15    | 0.00424    | 0.00113   | 0.04321   | 0.01010  | 230.34282  |
| 16    | 0.00348    | 0.00090   | 0.04283   | 0.01030  | 226.27458  |
| 17    | 0.00281    | 0.00062   | 0.04210   | 0.01000  | 228.54181  |
| 18    | 0.00234    | 0.00038   | 0.04137   | 0.00950  | 232.39875  |
| 19    | 0.00204    | 0.00028   | 0.04076   | 0.00930  | 228.27878  |